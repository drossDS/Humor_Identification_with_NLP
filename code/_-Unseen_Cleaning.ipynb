{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da67c024-8ef6-4269-a3b1-7c9d13944028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x2756d555700>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import html\n",
    "\n",
    "\n",
    "# Language Detection Imports:\n",
    "# SOURCE:  https://spacy.io/\n",
    "# SOURCE:  https://pypi.org/project/spacy-langdetect/\n",
    "import spacy\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.language import Language\n",
    "\n",
    "# Code provided from the following source:  https://stackoverflow.com/questions/66433496/how-do-i-fix-valueerror-when-doing-nlp-add-pipelanguagedetector-name-langua\n",
    "#  The documentation was providing code that was not working, and the above stack overflow post managed to fix the issue\n",
    "# SOURCE:  https://pypi.org/project/spacy-langdetect/\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def create_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "Language.factory(\"language_detector\", func=create_lang_detector)\n",
    "nlp.add_pipe('language_detector', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3320b3-b385-49ab-a67d-133fa9deb02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onion = pd.read_csv('../data/theonion_1682378516.csv')\n",
    "df_wldnws = pd.read_csv('../data/worldnews_1682378600.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2affcc43-e8bc-4e75-9522-2f9d0cca2569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onion_wldnws_cleaner(onion_df, wldnws_df, file_name):\n",
    "    \n",
    "    # DROP DUPLICATE TITLES\n",
    "    onion_df.drop_duplicates(subset = 'title', inplace=True)\n",
    "    wldnws_df.drop_duplicates(subset = 'title', inplace=True)\n",
    "    \n",
    "    # DROP UNNEEDED COLUMNS\n",
    "    onion_df = onion_df[['subreddit', 'title']]\n",
    "    wldnws_df = wldnws_df[['subreddit', 'title']]\n",
    "    \n",
    "    # FIX HTML ERRORS\n",
    "    wldnws_df['title'] = wldnws_df['title'].apply(html.unescape)\n",
    "    onion_df['title'] = onion_df['title'].apply(html.unescape)\n",
    "    \n",
    "    # REMOVE LOWERCASE ARTICLES\n",
    "    bad_onion_dfs = list(onion_df[onion_df['title'].str.islower() == True].index)\n",
    "    bad_news_dfs = list(wldnws_df[wldnws_df['title'].str.islower() == True].index)\n",
    "    onion_df.drop(index=bad_onion_dfs, inplace=True)\n",
    "    wldnws_df.drop(index=bad_news_dfs, inplace=True)\n",
    "    \n",
    "    # REMOVE NON_ENGLISH\n",
    "    wldnws_df['lang'] = wldnws_df['title'].apply(lambda x: nlp(x)._.language['language'])\n",
    "    wldnws_df = wldnws_df[wldnws_df['lang'] == 'en']\n",
    "    onion_df['lang'] = onion_df['title'].apply(lambda x: nlp(x)._.language['language'])\n",
    "    onion_df = onion_df[onion_df['lang'] == 'en']\n",
    "    \n",
    "    # CREATE CONCATENATED DATAFRAME\n",
    "    reddit_data = pd.concat([onion_df[['subreddit', 'title']], wldnws_df[['subreddit', 'title']]]).reset_index(drop = True)\n",
    "\n",
    "    \n",
    "# ==============================================================================================    \n",
    "    \n",
    "    # DROP TITLES REFERENCING SUBREDDIT NAME\n",
    "    # Find all titles with some variation of World News\n",
    "        # The regex code here was created with the help of the following sources:\n",
    "        # Source:  https://stackoverflow.com/questions/18402416/regular-expression-to-match-a-word-or-its-prefix\n",
    "        # Source:  https://stackoverflow.com/questions/20462834/python-using-str-replace-with-a-wildcard\n",
    "        # Source:  https://regex101.com/\n",
    "        # Source:  https://stackoverflow.com/questions/5633533/regular-expression-for-matching-parentheses\n",
    "        # Source:  https://stackoverflow.com/questions/4007302/regex-how-to-match-an-optional-character\n",
    "        # Source:  https://www.regular-expressions.info/optional.html\n",
    "        # Source:  https://stackoverflow.com/questions/9655164/regex-ignore-case-sensitivity\n",
    "        # Source:  https://stackoverflow.com/questions/34583904/javascript-regex-ignore-case-for-specific-capture-group\n",
    "        # Source:  https://stackoverflow.com/questions/7548787/regex-for-and-not-operation\n",
    "    '''\n",
    "    The regex expression below will (in order or characters left to right):\n",
    "    - outer () - says to match all inside the parentheses\n",
    "    - /{0,1}r{0,1}/{0,1} - find a string with or without each of  /, r, and /\n",
    "    - [wW]orld[nN]ews - find worldnew optionally capitalized\n",
    "    - ( Live Thread: )? - it may have this text after it, if so find it, if not, don't\n",
    "    - | either or\n",
    "    - outer () - says to match all inside the parenthese\n",
    "    - \\( - backslash tells it to treat left parenthesis as character\n",
    "    - Thread # - where it says 'Thread #'\n",
    "    - \\d{1,} - \\d any digit, {1,} at least one character long\n",
    "    - \\) - backslash tells it to treat right parenthesis as character\n",
    "    '''\n",
    "    # Create the regex string\n",
    "    regex_string = '(/{0,1}r{0,1}/{0,1}[wW]orld[nN]ews( Live Thread: )?)|( \\(Thread #\\d{1,}\\))'\n",
    "    bad_titles_ind = list(reddit_data[reddit_data['title'].apply(lambda title: len(re.findall(regex_string, title))) > 0].index)\n",
    "    # Append the full list of articles containing 'onion' identified above to the bad_titles list\n",
    "    for title in list(reddit_data[reddit_data['title'].str.lower().str.find('onion') > -1].index):\n",
    "        bad_titles_ind.append(title)\n",
    "    reddit_data.drop(index=bad_titles_ind, inplace=True);\n",
    "    \n",
    "    # REMOVE HASHTAGS\n",
    "    reddit_data['title'] = reddit_data['title'].apply(lambda title: title.replace('&#x27;',''))\n",
    "    # Find all titles with some variation of World News\n",
    "    '''\n",
    "    The regex string below looks to find and # followed by a word and not followed by a number\n",
    "    '''\n",
    "    # Create the regex string from before\n",
    "    regex_string = '#\\D\\w+'\n",
    "    reddit_data[reddit_data['title'].apply(lambda title: len(re.findall(regex_string, title))) > 0]\n",
    "    reddit_data['title'] = reddit_data['title'].apply(lambda title: re.sub(regex_string, '', title));\n",
    "    \n",
    "    # REMOVED NEW ARTICLE TITLES USUALLY AFTER PIPES AND DASHES\n",
    "    # For the PIPES\n",
    "    regex_string = '\\| [\\w \\W]+'\n",
    "    reddit_data['title'] = reddit_data['title'].apply(lambda title: re.sub(regex_string, '', title))\n",
    "    # For the DASHES\n",
    "    regex_string = '\\- [\\w \\W]+'\n",
    "    reddit_data['title'] = reddit_data['title'].apply(lambda title: re.sub(regex_string, '', title))\n",
    "    \n",
    "    #  REMOVE EMOJIS AND SPECIAL CHARACTERS\n",
    "    # Test the regex expression to remove emojis:\n",
    "        # This expression will remove emojis and most punctuation which won't be necessary for the algorithms used.\n",
    "        # Source to help with this code:  https://stackoverflow.com/questions/7548787/regex-for-and-not-operation\n",
    "    '''\n",
    "    This regex will find all non word items and exclude all of the following characters (a space is the first character):  ' &$%\\-#/'\n",
    "    The characters selected to be excluded above were iteratively chosen by examining the outputs of the find all code below.  \n",
    "    The word vectorizers will do the vast majority of the punctuation removal, but here, these characters are being skipped \n",
    "    as replacing them with nothing may change the meaning of some words or combine two words that shoud not be together.\n",
    "    '''\n",
    "    regex_string = \"((?=[^ &$%\\-#/])\\W)\"\n",
    "    reddit_data['title'] = reddit_data['title'].apply(lambda title: re.sub(regex_string, '', title))\n",
    "    \n",
    "    # Remove Starting and Trailing Spaces\n",
    "    reddit_data['title'] = reddit_data['title'].apply(lambda title: title.strip())\n",
    "    \n",
    "    # REMOVE TITLES WITH TWO OR FEWER WORDS\n",
    "    reddit_data.drop(index = reddit_data[reddit_data['title'].apply(lambda x: len(x.strip().split())) <= 2].index,\n",
    "           inplace=True)\n",
    "    \n",
    "    # Reset the index:\n",
    "    reddit_data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # Export to a csv\n",
    "    reddit_data.to_csv(f'../data/{file_name}.csv', index=False)\n",
    "        \n",
    "    return reddit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e88ba12a-8507-4607-8012-5d80dff68214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\1771726477.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wldnws_df['title'] = wldnws_df['title'].apply(html.unescape)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\1771726477.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  onion_df['title'] = onion_df['title'].apply(html.unescape)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\1771726477.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  onion_df.drop(index=bad_onion_dfs, inplace=True)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\1771726477.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wldnws_df.drop(index=bad_news_dfs, inplace=True)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\1771726477.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wldnws_df['lang'] = wldnws_df['title'].apply(lambda x: nlp(x)._.language['language'])\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\1771726477.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  onion_df['lang'] = onion_df['title'].apply(lambda x: nlp(x)._.language['language'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>US Schools Trail World In Child Soldier Aptitude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>Is Your Flamingo Sick Enough To Make A Movie A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>More American Workers Outsourcing Own Jobs Ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>So People Could Be Listening To This Conversat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>Destroyed substitute teachers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>For the first time renewable energy generation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>Chinas loans to Africa worry World Bank Presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>In Ukraine where even the corpses are booby tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>Russian aggression killed 262 Ukrainian athletes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>EC recognizes Filipino seafarers certificates ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1732 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit                                              title\n",
       "0      TheOnion   US Schools Trail World In Child Soldier Aptitude\n",
       "1      TheOnion  Is Your Flamingo Sick Enough To Make A Movie A...\n",
       "2      TheOnion  More American Workers Outsourcing Own Jobs Ove...\n",
       "3      TheOnion  So People Could Be Listening To This Conversat...\n",
       "4      TheOnion                      Destroyed substitute teachers\n",
       "...         ...                                                ...\n",
       "1727  worldnews  For the first time renewable energy generation...\n",
       "1728  worldnews  Chinas loans to Africa worry World Bank Presid...\n",
       "1729  worldnews  In Ukraine where even the corpses are booby tr...\n",
       "1730  worldnews   Russian aggression killed 262 Ukrainian athletes\n",
       "1731  worldnews  EC recognizes Filipino seafarers certificates ...\n",
       "\n",
       "[1732 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onion_test = pd.read_csv('../data/theonion_1578009619.csv')\n",
    "df_wldnws_test = pd.read_csv('../data/worldnews_1680688103.csv')\n",
    "\n",
    "df = onion_wldnws_cleaner(df_onion_test, df_wldnws_test, 'reddit_holdout_2')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e2fd1-f817-45c2-a5f3-89babdeacf5b",
   "metadata": {},
   "source": [
    "# <font color = 'red'> DONT FORGET TO RESET INDICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf4d2f04-1f04-4932-8875-890c8f8ff884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\597679520.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wldnws['title'] = wldnws['title'].apply(html.unescape)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\597679520.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  onion['title'] = onion['title'].apply(html.unescape)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\597679520.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  onion.drop(index=bad_onions, inplace=True)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\597679520.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wldnws.drop(index=bad_news, inplace=True)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\597679520.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wldnws['lang'] = wldnws['title'].apply(lambda x: nlp(x)._.language['language'])\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\597679520.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  onion['lang'] = onion['title'].apply(lambda x: nlp(x)._.language['language'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>Idiot Tornado Tears Harmlessly Through Empty F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>New Texas Law Requires Schools To Display Imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>New Poll Finds Americans Would Respect Biden M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>Could You Pass Racial Discrimination Training ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>Dog And Owner Having Public Fight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10360</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>Diners in Japan arrested for dipping own chops...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10361</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>200 Russian Journalists Sign Letter Demanding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10362</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>Foxconn founder Gou to run for Taiwan presiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10363</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>A wartime NATO struggles to replace its chief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10364</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>Italys government wants to ban English with fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10045 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit                                              title\n",
       "0       TheOnion  Idiot Tornado Tears Harmlessly Through Empty F...\n",
       "1       TheOnion  New Texas Law Requires Schools To Display Imag...\n",
       "2       TheOnion  New Poll Finds Americans Would Respect Biden M...\n",
       "3       TheOnion  Could You Pass Racial Discrimination Training ...\n",
       "4       TheOnion                  Dog And Owner Having Public Fight\n",
       "...          ...                                                ...\n",
       "10360  worldnews  Diners in Japan arrested for dipping own chops...\n",
       "10361  worldnews  200 Russian Journalists Sign Letter Demanding ...\n",
       "10362  worldnews  Foxconn founder Gou to run for Taiwan presiden...\n",
       "10363  worldnews      A wartime NATO struggles to replace its chief\n",
       "10364  worldnews  Italys government wants to ban English with fi...\n",
       "\n",
       "[10045 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = onion_wldnws_cleaner(df_onion, df_wldnws, )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f94b298f-55b2-4526-a845-93ea0e1bc7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\406774544.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wldnws['title'] = wldnws['title'].apply(html.unescape)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\406774544.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  onion['title'] = onion['title'].apply(html.unescape)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\406774544.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  onion.drop(index=bad_onions, inplace=True)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\406774544.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wldnws.drop(index=bad_news, inplace=True)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\406774544.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wldnws['lang'] = wldnws['title'].apply(lambda x: nlp(x)._.language['language'])\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\406774544.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  onion['lang'] = onion['title'].apply(lambda x: nlp(x)._.language['language'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>Idiot Tornado Tears Harmlessly Through Empty F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>New Texas Law Requires Schools To Display Imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>New Poll Finds Americans Would Respect Biden M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>Could You Pass Racial Discrimination Training ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>Dog And Owner Having Public Fight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10366</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>Diners in Japan arrested for dipping own chops...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10367</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>200 Russian Journalists Sign Letter Demanding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10368</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>Foxconn founder Gou to run for Taiwan presiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10369</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>A wartime NATO struggles to replace its chief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10370</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>Italys government wants to ban English with fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10134 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit                                              title\n",
       "0       TheOnion  Idiot Tornado Tears Harmlessly Through Empty F...\n",
       "1       TheOnion  New Texas Law Requires Schools To Display Imag...\n",
       "2       TheOnion  New Poll Finds Americans Would Respect Biden M...\n",
       "3       TheOnion  Could You Pass Racial Discrimination Training ...\n",
       "4       TheOnion                  Dog And Owner Having Public Fight\n",
       "...          ...                                                ...\n",
       "10366  worldnews  Diners in Japan arrested for dipping own chops...\n",
       "10367  worldnews  200 Russian Journalists Sign Letter Demanding ...\n",
       "10368  worldnews  Foxconn founder Gou to run for Taiwan presiden...\n",
       "10369  worldnews      A wartime NATO struggles to replace its chief\n",
       "10370  worldnews  Italys government wants to ban English with fi...\n",
       "\n",
       "[10134 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = onion_wldnws_cleaner(df_onion, df_wldnws, )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0b703d4-6f2d-4310-996a-244aa057c388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\3178922256.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wldnws['title'] = wldnws['title'].apply(html.unescape)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\3178922256.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  onion['title'] = onion['title'].apply(html.unescape)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\3178922256.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  onion.drop(index=bad_onions, inplace=True)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\3178922256.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wldnws.drop(index=bad_news, inplace=True)\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\3178922256.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wldnws['lang'] = wldnws['title'].apply(lambda x: nlp(x)._.language['language'])\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_7252\\3178922256.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  onion['lang'] = onion['title'].apply(lambda x: nlp(x)._.language['language'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>US Schools Trail World In Child Soldier Aptitude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>Is Your Flamingo Sick Enough To Make A Movie A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>More American Workers Outsourcing Own Jobs Ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>So People Could Be Listening To This Conversat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TheOnion</td>\n",
       "      <td>Destroyed substitute teachers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>For the first time renewable energy generation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>Chinas loans to Africa worry World Bank Presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>In Ukraine where even the corpses are booby tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>Russian aggression killed 262 Ukrainian athletes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>EC recognizes Filipino seafarers certificates ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1729 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit                                              title\n",
       "0      TheOnion   US Schools Trail World In Child Soldier Aptitude\n",
       "1      TheOnion  Is Your Flamingo Sick Enough To Make A Movie A...\n",
       "2      TheOnion  More American Workers Outsourcing Own Jobs Ove...\n",
       "3      TheOnion  So People Could Be Listening To This Conversat...\n",
       "4      TheOnion                      Destroyed substitute teachers\n",
       "...         ...                                                ...\n",
       "1724  worldnews  For the first time renewable energy generation...\n",
       "1725  worldnews  Chinas loans to Africa worry World Bank Presid...\n",
       "1726  worldnews  In Ukraine where even the corpses are booby tr...\n",
       "1727  worldnews   Russian aggression killed 262 Ukrainian athletes\n",
       "1728  worldnews  EC recognizes Filipino seafarers certificates ...\n",
       "\n",
       "[1729 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onion_test = pd.read_csv('../data/theonion_1578009619.csv')\n",
    "df_wldnws_test = pd.read_csv('../data/worldnews_1680688103.csv')\n",
    "\n",
    "df = onion_wldnws_cleaner(df_onion_test, df_wldnws_test, 'reddit_holdout_2')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a3bdffe-393d-424c-9a0f-61c40ae0430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/holdout_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925a08c5-49a7-4575-bcff-794aa40d6fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216b23c-5355-4b1b-a722-70c79cfc7621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 1.1.2 - DROP DUPLICATE TITLES\n",
    "\n",
    "onion.drop_duplicates(subset = 'title', inplace=True)\n",
    "wldnws.drop_duplicates(subset = 'title', inplace=True)\n",
    "onion.shape, wldnws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bbc43-6ec1-48ea-bdce-6ca7d67c9f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 1.3 - DROP UNNEEDED COLUMNS\n",
    "onion = onion[['subreddit', 'title']]\n",
    "wldnws = wldnws[['subreddit', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f6e70-6f23-4e80-a8f5-623d8744756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 2.1 - FIX HTML ERRORS\n",
    "wldnws['title'] = wldnws['title'].apply(html.unescape)\n",
    "onion['title'] = onion['title'].apply(html.unescape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a0832-eae4-4754-a3dd-e7c3c4c60ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 2.4 - REMOVE LOWERCASE ARTICLES\n",
    "bad_onions = list(onion[onion['title'].str.islower() == True].index)\n",
    "bad_news = list(wldnws[wldnws['title'].str.islower() == True].index)\n",
    "\n",
    "onion.drop(index=bad_onions, inplace=True)\n",
    "wldnws.drop(index=bad_news, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec83cd9-d3e2-441b-a081-43694de17cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 3 - Remove non-English\n",
    "\n",
    "# Code provided from the following source:  https://stackoverflow.com/questions/66433496/how-do-i-fix-valueerror-when-doing-nlp-add-pipelanguagedetector-name-langua\n",
    "#  The documentation was providing code that was not working, and the above stack overflow post managed to fix the issue\n",
    "# SOURCE:  https://pypi.org/project/spacy-langdetect/\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def create_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "Language.factory(\"language_detector\", func=create_lang_detector)\n",
    "nlp.add_pipe('language_detector', last=True)\n",
    "\n",
    "wldnws['lang'] = wldnws['title'].apply(lambda x: nlp(x)._.language['language'])\n",
    "wldnws.head(2)\n",
    "\n",
    "wldnws = wldnws[wldnws['lang'] == 'en']\n",
    "\n",
    "onion['lang'] = onion['title'].apply(lambda x: nlp(x)._.language['language'])\n",
    "onion.head(2)\n",
    "\n",
    "onion = onion[onion['lang'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f7469-a12d-4930-9122-df130eb88c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - 4.3 - CREATE CONCATENATED DATAFRAME\n",
    "reddit_df = pd.concat([onion[['subreddit', 'title']], wldnws[['subreddit', 'title']]]).reset_index(drop = True)\n",
    "print(reddit_df.shape)\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddfd575-7ce9-4353-a7e8-8eb9e408a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - 4.1 - DROP TITLES REFERENCING SUBREDDIT NAME\n",
    "\n",
    "# Find all titles with some variation of World News\n",
    "\n",
    "# The regex code here was created with the help of the following sources:\n",
    "# Source:  https://stackoverflow.com/questions/18402416/regular-expression-to-match-a-word-or-its-prefix\n",
    "# Source:  https://stackoverflow.com/questions/20462834/python-using-str-replace-with-a-wildcard\n",
    "# Source:  https://regex101.com/\n",
    "# Source:  https://stackoverflow.com/questions/5633533/regular-expression-for-matching-parentheses\n",
    "# Source:  https://stackoverflow.com/questions/4007302/regex-how-to-match-an-optional-character\n",
    "# Source:  https://www.regular-expressions.info/optional.html\n",
    "# Source:  https://stackoverflow.com/questions/9655164/regex-ignore-case-sensitivity\n",
    "# Source:  https://stackoverflow.com/questions/34583904/javascript-regex-ignore-case-for-specific-capture-group\n",
    "# Source:  https://stackoverflow.com/questions/7548787/regex-for-and-not-operation\n",
    "\n",
    "'''\n",
    "The regex expression below will (in order or characters left to right):\n",
    "- outer () - says to match all inside the parentheses\n",
    "- /{0,1}r{0,1}/{0,1} - find a string with or without each of  /, r, and /\n",
    "- [wW]orld[nN]ews - find worldnew optionally capitalized\n",
    "- ( Live Thread: )? - it may have this text after it, if so find it, if not, don't\n",
    "- | either or\n",
    "- outer () - says to match all inside the parenthese\n",
    "- \\( - backslash tells it to treat left parenthesis as character\n",
    "- Thread # - where it says 'Thread #'\n",
    "- \\d{1,} - \\d any digit, {1,} at least one character long\n",
    "- \\) - backslash tells it to treat right parenthesis as character\n",
    "'''\n",
    "\n",
    "# Create the regex string\n",
    "regex_string = '(/{0,1}r{0,1}/{0,1}[wW]orld[nN]ews( Live Thread: )?)|( \\(Thread #\\d{1,}\\))'\n",
    "bad_titles = list(reddit[reddit['title'].apply(lambda title: len(re.findall(regex_string, title))) > 0].index)\n",
    "\n",
    "# Append the full list of articles containing 'onion' identified above to the bad_titles list\n",
    "\n",
    "for title in list(reddit[reddit['title'].str.lower().str.find('onion') > -1].index):\n",
    "    bad_titles.append(title)\n",
    "\n",
    "reddit.drop(index=bad_titles, inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc761e7-16d2-4cb1-994e-07f25d85bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - 4.2 - Remove Hashtags:\n",
    "reddit['title'] = reddit['title'].apply(lambda title: title.replace('&#x27;',''))\n",
    "\n",
    "# Find all titles with some variation of World News\n",
    "\n",
    "'''\n",
    "The regex string below looks to find and # followed by a word and not followed by a number\n",
    "\n",
    "'''\n",
    "\n",
    "# Create the regex string from before\n",
    "regex_string = '#\\D\\w+'\n",
    "\n",
    "reddit[reddit['title'].apply(lambda title: len(re.findall(regex_string, title))) > 0]\n",
    "\n",
    "reddit['title'] = reddit['title'].apply(lambda title: re.sub(regex_string, '', title));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64190470-1ec8-44f9-be31-d9fd3ce076bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - 4.3 - REMOVED NEW ARTICLE TITLES USUALLY AFTER PIPES AND DASHES\n",
    "\n",
    "# For the PIPES\n",
    "regex_string = '\\| [\\w \\W]+'\n",
    "\n",
    "reddit['title'] = reddit['title'].apply(lambda title: re.sub(regex_string, '', title))\n",
    "\n",
    "# For the DASHES\n",
    "regex_string = '\\- [\\w \\W]+'\n",
    "\n",
    "reddit['title'] = reddit['title'].apply(lambda title: re.sub(regex_string, '', title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ab7a2-fda1-45ed-83dc-482618530fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - 4.4 - REMOVE EMOJIS AND SPECIAL CHARACTERS\n",
    "\n",
    "# Test the regex expression to remove emojis:\n",
    "\n",
    "# This expression will remove emojis and most punctuation which won't be necessary for the algorithms used.\n",
    "# Source to help with this code:  https://stackoverflow.com/questions/7548787/regex-for-and-not-operation\n",
    "'''\n",
    "This regex will find all non word items and exclude all of the following characters (a space is the first character):  ' &$%\\-#/'\n",
    "\n",
    "The characters selected to be excluded above were iteratively chosen by examining the outputs of the find all code below.  \n",
    "The word vectorizers will do the vast majority of the punctuation removal, but here, these characters are being skipped \n",
    "as replacing them with nothing may change the meaning of some words or combine two words that shoud not be together.\n",
    "\n",
    "'''\n",
    "#regex_string2 = \"((?=[^ &$%\\-#/])\\W)\"\n",
    "regex_string = \"((?=[^ &$%\\-#/])\\W)\"\n",
    "[re.findall(regex_string, x) for x in reddit.title if len(re.findall(regex_string, x))>0]\n",
    "\n",
    "reddit['title'] = reddit['title'].apply(lambda title: re.sub(regex_string, '', title))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07dbd9-c7d0-4531-a20d-a19ac2681f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - 4.5 - Remove Starting and Trailing Spaces\n",
    "reddit['title'] = reddit['title'].apply(lambda title: title.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7098e-775d-4d3c-90ab-42e1cb5da09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc78537e-efb0-4174-a2c1-96f77471b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onion_wldnws_cleaner(onion_df, wldnws, file_name):\n",
    "    \n",
    "    # DROP DUPLICATE TITLES\n",
    "    onion_df.drop_duplicates(subset = 'title', inplace=True)\n",
    "    wldnws.drop_duplicates(subset = 'title', inplace=True)\n",
    "    \n",
    "    # DROP UNNEEDED COLUMNS\n",
    "    onion_df = onion_df[['subreddit', 'title']]\n",
    "    wldnws = wldnws[['subreddit', 'title']]\n",
    "    \n",
    "    # FIX HTML ERRORS\n",
    "    wldnws['title'] = wldnws['title'].apply(html.unescape)\n",
    "    onion_df['title'] = onion_df['title'].apply(html.unescape)\n",
    "    \n",
    "    # REMOVE LOWERCASE ARTICLES\n",
    "    bad_onion_dfs = list(onion_df[onion_df['title'].str.islower() == True].index)\n",
    "    bad_news = list(wldnws[wldnws['title'].str.islower() == True].index)\n",
    "    onion_df.drop(index=bad_onion_dfs, inplace=True)\n",
    "    wldnws.drop(index=bad_news, inplace=True)\n",
    "    \n",
    "    # REMOVE NON_ENGLISH\n",
    "    wldnws['lang'] = wldnws['title'].apply(lambda x: nlp(x)._.language['language'])\n",
    "    wldnws.head(2)\n",
    "\n",
    "    wldnws = wldnws[wldnws['lang'] == 'en']\n",
    "\n",
    "    onion_df['lang'] = onion_df['title'].apply(lambda x: nlp(x)._.language['language'])\n",
    "    onion_df.head(2)\n",
    "\n",
    "    onion_df = onion_df[onion_df['lang'] == 'en']\n",
    "    \n",
    "    # CREATE CONCATENATED DATAFRAME\n",
    "    reddit = pd.concat([onion_df[['subreddit', 'title']], wldnws[['subreddit', 'title']]]).reset_index(drop = True)\n",
    "\n",
    "    \n",
    "# ==============================================================================================    \n",
    "    \n",
    "    # DROP TITLES REFERENCING SUBREDDIT NAME\n",
    "    # Find all titles with some variation of World News\n",
    "        # The regex code here was created with the help of the following sources:\n",
    "        # Source:  https://stackoverflow.com/questions/18402416/regular-expression-to-match-a-word-or-its-prefix\n",
    "        # Source:  https://stackoverflow.com/questions/20462834/python-using-str-replace-with-a-wildcard\n",
    "        # Source:  https://regex101.com/\n",
    "        # Source:  https://stackoverflow.com/questions/5633533/regular-expression-for-matching-parentheses\n",
    "        # Source:  https://stackoverflow.com/questions/4007302/regex-how-to-match-an-optional-character\n",
    "        # Source:  https://www.regular-expressions.info/optional.html\n",
    "        # Source:  https://stackoverflow.com/questions/9655164/regex-ignore-case-sensitivity\n",
    "        # Source:  https://stackoverflow.com/questions/34583904/javascript-regex-ignore-case-for-specific-capture-group\n",
    "        # Source:  https://stackoverflow.com/questions/7548787/regex-for-and-not-operation\n",
    "    '''\n",
    "    The regex expression below will (in order or characters left to right):\n",
    "    - outer () - says to match all inside the parentheses\n",
    "    - /{0,1}r{0,1}/{0,1} - find a string with or without each of  /, r, and /\n",
    "    - [wW]orld[nN]ews - find worldnew optionally capitalized\n",
    "    - ( Live Thread: )? - it may have this text after it, if so find it, if not, don't\n",
    "    - | either or\n",
    "    - outer () - says to match all inside the parenthese\n",
    "    - \\( - backslash tells it to treat left parenthesis as character\n",
    "    - Thread # - where it says 'Thread #'\n",
    "    - \\d{1,} - \\d any digit, {1,} at least one character long\n",
    "    - \\) - backslash tells it to treat right parenthesis as character\n",
    "    '''\n",
    "    # Create the regex string\n",
    "    regex_string = '(/{0,1}r{0,1}/{0,1}[wW]orld[nN]ews( Live Thread: )?)|( \\(Thread #\\d{1,}\\))'\n",
    "    bad_titles = list(reddit[reddit['title'].apply(lambda title: len(re.findall(regex_string, title))) > 0].index)\n",
    "    # Append the full list of articles containing 'onion' identified above to the bad_titles list\n",
    "    for title in list(reddit[reddit['title'].str.lower().str.find('onion') > -1].index):\n",
    "        bad_titles.append(title)\n",
    "    reddit.drop(index=bad_titles, inplace=True);\n",
    "    \n",
    "    # REMOVE HASHTAGS\n",
    "    reddit['title'] = reddit['title'].apply(lambda title: title.replace('&#x27;',''))\n",
    "    # Find all titles with some variation of World News\n",
    "    '''\n",
    "    The regex string below looks to find and # followed by a word and not followed by a number\n",
    "    '''\n",
    "    # Create the regex string from before\n",
    "    regex_string = '#\\D\\w+'\n",
    "    reddit[reddit['title'].apply(lambda title: len(re.findall(regex_string, title))) > 0]\n",
    "    reddit['title'] = reddit['title'].apply(lambda title: re.sub(regex_string, '', title));\n",
    "    \n",
    "    # REMOVED NEW ARTICLE TITLES USUALLY AFTER PIPES AND DASHES\n",
    "    # For the PIPES\n",
    "    regex_string = '\\| [\\w \\W]+'\n",
    "    reddit['title'] = reddit['title'].apply(lambda title: re.sub(regex_string, '', title))\n",
    "    # For the DASHES\n",
    "    regex_string = '\\- [\\w \\W]+'\n",
    "    reddit['title'] = reddit['title'].apply(lambda title: re.sub(regex_string, '', title))\n",
    "    \n",
    "    #  REMOVE EMOJIS AND SPECIAL CHARACTERS\n",
    "    # Test the regex expression to remove emojis:\n",
    "        # This expression will remove emojis and most punctuation which won't be necessary for the algorithms used.\n",
    "        # Source to help with this code:  https://stackoverflow.com/questions/7548787/regex-for-and-not-operation\n",
    "    '''\n",
    "    This regex will find all non word items and exclude all of the following characters (a space is the first character):  ' &$%\\-#/'\n",
    "    The characters selected to be excluded above were iteratively chosen by examining the outputs of the find all code below.  \n",
    "    The word vectorizers will do the vast majority of the punctuation removal, but here, these characters are being skipped \n",
    "    as replacing them with nothing may change the meaning of some words or combine two words that shoud not be together.\n",
    "    '''\n",
    "    regex_string = \"((?=[^ &$%\\-#/])\\W)\"\n",
    "    reddit['title'] = reddit['title'].apply(lambda title: re.sub(regex_string, '', title))\n",
    "    \n",
    "    # Remove Starting and Trailing Spaces\n",
    "    reddit['title'] = reddit['title'].apply(lambda title: title.strip())\n",
    "    \n",
    "    # REMOVE TITLES WITH TWO OR FEWER WORDS\n",
    "    reddit.drop(index = reddit[reddit['title'].apply(lambda x: len(x.strip().split())) <= 2].index,\n",
    "           inplace=True)\n",
    "    \n",
    "    # Reset the index:\n",
    "    reddit.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    # Export to a csv\n",
    "    reddit.to_csv(f'../data/{file_name}.csv', index=False)\n",
    "        \n",
    "    return reddit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
