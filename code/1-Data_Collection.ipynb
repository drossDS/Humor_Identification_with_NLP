{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec24a5a-3cd1-488d-a458-a3d27a5f4476",
   "metadata": {},
   "source": [
    "# 1 - Project 3 Data Collection\n",
    "This is the first of a series of notebooks for this project.\n",
    "\n",
    "Note that subsquent project notebooks will refer to this notebook.  As running this notebook will provide data for a fixed point in time, and could potentially over-write data collected previously, this will be treated as a stand-alone notebook.  Timestamps will be printed to a csv so that this code can be modified to duplicate the run shown in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd91bca-d285-464a-89b4-c55e7a7a69a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1910472-3e56-46c3-98f3-81e1336ad9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51011f-d4f2-44f8-b05a-ed663898c5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1db7c-b4a9-4c97-ab07-e4018f92f7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa8e9b8-51b3-43c9-ad24-b9b3dfd1e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Sourece to get current Unix Timestamp for the pushshift API:\n",
    "#  https://stackoverflow.com/questions/16755394/what-is-the-easiest-way-to-get-current-gmt-time-in-unix-timestamp-format\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83910bc-bc48-4981-a161-a66fc87dd789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "3bc46a4a-bb1a-4812-ab9b-040094f9ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_getter(subreddit, trials, name):\n",
    "#=====  INITIAL LOCAL VARIABLES  =================================================================================\n",
    "    \n",
    "    # Establish base url:\n",
    "    url_fnc = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    \n",
    "    # Establish initial parameters for most current subreddit pull:\n",
    "    params_getter = {\n",
    "    'subreddit': subreddit,\n",
    "    'size': 1000,\n",
    "    }\n",
    "    \n",
    "    # Create an empty dataframe to which we can concatenate each run\n",
    "    master_df = pd.DataFrame()\n",
    "    \n",
    "#=====  Gathering the data from reddit/pushshift // Create DataFrame // Concatenate to Master  ===================\n",
    "    \n",
    "    # Get the data:\n",
    "    res_fnc = requests.get(url_fnc, params_getter)\n",
    "    print(res_fnc.status_code) # for debugging\n",
    "    \n",
    "    # Make dataframe:\n",
    "    df = res_fnc.json() # Dump dat to a json\n",
    "    df = pd.DataFrame(df['data']) # Pull the 'data' dictionary out of the json\n",
    "    \n",
    "    # Concatenate to master\n",
    "    master_df = pd.concat([master_df, df])\n",
    "\n",
    "#=====  Iterate the above steps over remaining trials  ===========================================================\n",
    "    for i in range(0, (trials - 1)): # Establishes a for loop to iterate over the remaining trials (minus the first one)\n",
    "        \n",
    "        # Update the before parameter to be the retrieved time (in utc) of the last item in the previous trail's dataframe\n",
    "        params_getter = {\n",
    "        'subreddit': subreddit,\n",
    "        'size': 1000,\n",
    "        'before': list(df['retrieved_utc'][-1:])[0]\n",
    "        }\n",
    "        \n",
    "        # Get the data:\n",
    "        res_fnc = requests.get(url_fnc, params_getter)\n",
    "        print(res_fnc.status_code) # for debugging\n",
    "\n",
    "        # Make dataframe:\n",
    "        df = res_fnc.json() # Dump dat to a json\n",
    "        df = pd.DataFrame(df['data']) # Pull the 'data' dictionary out of the json\n",
    "\n",
    "        # Concatenate to master\n",
    "        master_df = pd.concat([master_df, df])\n",
    "    \n",
    "#=====  Finally, return the fully concatenated master dataframe, reset index, store to csv  =================================================\n",
    "    master_df.reset_index(drop = True) # Source for refersher on how to tuse this:  https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html\n",
    "    master_df.to_csv(f'./data/{name}_{time.time()}_.csv')\n",
    "    return master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d4d28-c982-44c9-8e78-8279bd5ef984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "357e9e8f-4272-4655-92f5-0f11c453ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:  https://www.guru99.com/reading-and-writing-files-in-python.html\n",
    "f= open(\"testing.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2760bda-b979-4d06-8e2e-71c735fbf9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
